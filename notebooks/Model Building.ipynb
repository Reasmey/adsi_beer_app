{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model building\n",
    "\n",
    "https://www.kaggle.com/vadbeg/pytorch-nn-with-embeddings-and-catboost/notebook#PyTorch\n",
    "\n",
    "mostly based off this example, plus parts of code form tutorial 5 lab 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import load_data function from \n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "# fix system path\n",
    "import sys\n",
    "sys.path.append(\"/home/jovyan/work\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from torch.utils.data import Dataset, DataLoader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import random\n",
    "\n",
    "def set_seed(seed):\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    \n",
    "    torch.manual_seed(seed)\n",
    "    torch.backends.cudnn.deterministick = True\n",
    "    torch.backends.cudnn.benchmark = False \n",
    "    \n",
    "set_seed(27)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.data.sets import load_sets\n",
    "\n",
    "X_train, y_train, X_val, y_val, X_test, y_test = load_sets()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(317320, 5)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_test.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(951959, 5)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_train.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(317320, 5)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "X_val.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# need to convert to tensors\n",
    "from src.models.pytorch import EmbeddingDataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = EmbeddingDataset(X_train, \n",
    "                                      targets=y_train,\n",
    "                                      cat_cols_idx=[0],\n",
    "                                      cont_cols_idx=[1,2,3,4])\n",
    "\n",
    "val_dataset = EmbeddingDataset(X_val, \n",
    "                                      targets=y_val,\n",
    "                                      cat_cols_idx=[0],\n",
    "                                      cont_cols_idx=[1,2,3,4])\n",
    "\n",
    "\n",
    "test_dataset = EmbeddingDataset(X_test,\n",
    "                                     cat_cols_idx=[0],\n",
    "                                     cont_cols_idx=[1,2,3,4],\n",
    "                                     is_train=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First element of train_dataset: {'data': [tensor([4918.]), tensor([-3.2047, -2.1777, -0.3572, -0.4001])], 'target': tensor(13)}\n",
      "First element of val_dataset: {'data': [tensor([163.]), tensor([-1.0545, -0.5545, -1.0901, -1.0832])], 'target': tensor(31)}\n",
      "First element of test_dataset: {'data': [tensor([701.]), tensor([ 0.3790,  0.2570,  0.3757, -0.4001])]}\n"
     ]
    }
   ],
   "source": [
    "print(f'First element of train_dataset: {train_dataset[1]}',\n",
    "      f'First element of val_dataset: {val_dataset[1]}',\n",
    "      f'First element of test_dataset: {test_dataset[1]}',sep='\\n')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 81,
   "metadata": {},
   "outputs": [],
   "source": [
    "# embedding example\n",
    "class ClassificationEmbdNN(torch.nn.Module):\n",
    "    \n",
    "    def __init__(self, emb_dims, no_of_cont=None):\n",
    "        super(ClassificationEmbdNN, self).__init__()\n",
    "        \n",
    "        self.emb_layers = torch.nn.ModuleList([torch.nn.Embedding(x, y)\n",
    "                                               for x, y in emb_dims])\n",
    "        \n",
    "        no_of_embs = sum([y for x, y in emb_dims])\n",
    "        self.no_of_embs = no_of_embs\n",
    "        self.emb_dropout = torch.nn.Dropout(0.2)\n",
    "        \n",
    "        self.no_of_cont = 0\n",
    "        if no_of_cont:\n",
    "            self.no_of_cont = no_of_cont\n",
    "            self.bn_cont = torch.nn.BatchNorm1d(no_of_cont)\n",
    "        \n",
    "        self.fc1 = torch.nn.Linear(in_features=self.no_of_embs + self.no_of_cont, \n",
    "                                   out_features=208)\n",
    "        self.dropout1 = torch.nn.Dropout(0.2)\n",
    "        self.bn1 = torch.nn.BatchNorm1d(208)\n",
    "        self.act1 = torch.nn.ReLU()\n",
    "        \n",
    "        self.fc2 = torch.nn.Linear(in_features=208, \n",
    "                                   out_features=208)\n",
    "        self.dropout2 = torch.nn.Dropout(0.2)\n",
    "        self.bn2 = torch.nn.BatchNorm1d(208)\n",
    "        self.act2 = torch.nn.ReLU()\n",
    "        \n",
    "#         self.fc3 = torch.nn.Linear(in_features=256, \n",
    "#                                    out_features=64)\n",
    "#         self.dropout3 = torch.nn.Dropout(0.2)\n",
    "#         self.bn3 = torch.nn.BatchNorm1d(64)\n",
    "#         self.act3 = torch.nn.ReLU()\n",
    "        \n",
    "        self.fc3 = torch.nn.Linear(in_features=208, \n",
    "                                   out_features=104)\n",
    "        self.act3 = torch.nn.Softmax()\n",
    "        \n",
    "    def forward(self, x_cat, x_cont=None):\n",
    "        if self.no_of_embs != 0:\n",
    "            x = [emb_layer(x_cat[:, i])\n",
    "                 for i, emb_layer in enumerate(self.emb_layers)]\n",
    "        \n",
    "            x = torch.cat(x, 1)\n",
    "            x = self.emb_dropout(x)\n",
    "            \n",
    "        if self.no_of_cont != 0:\n",
    "            x_cont = self.bn_cont(x_cont)\n",
    "            \n",
    "            if self.no_of_embs != 0:\n",
    "                x = torch.cat([x, x_cont], 1)\n",
    "            else:\n",
    "                x = x_cont\n",
    "        \n",
    "        x = self.fc1(x)\n",
    "        x = self.dropout1(x)\n",
    "        x = self.bn1(x)\n",
    "        x = self.act1(x)\n",
    "        \n",
    "        x = self.fc2(x)\n",
    "        x = self.dropout2(x)\n",
    "        x = self.bn2(x)\n",
    "        x = self.act2(x)\n",
    "        \n",
    "#         x = self.fc3(x)\n",
    "#         x = self.dropout3(x)\n",
    "#         x = self.bn3(x)\n",
    "#         x = self.act3(x)\n",
    "        \n",
    "        x = self.fc3(x)\n",
    "        x = self.act3(x)\n",
    "        \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = ClassificationEmbdNN(emb_dims=[[5742, 252]], \n",
    "                             no_of_cont=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "ClassificationEmbdNN(\n",
       "  (emb_layers): ModuleList(\n",
       "    (0): Embedding(5742, 252)\n",
       "  )\n",
       "  (emb_dropout): Dropout(p=0.2, inplace=False)\n",
       "  (bn_cont): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (fc1): Linear(in_features=256, out_features=208, bias=True)\n",
       "  (dropout1): Dropout(p=0.2, inplace=False)\n",
       "  (bn1): BatchNorm1d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act1): ReLU()\n",
       "  (fc2): Linear(in_features=208, out_features=208, bias=True)\n",
       "  (dropout2): Dropout(p=0.2, inplace=False)\n",
       "  (bn2): BatchNorm1d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
       "  (act2): ReLU()\n",
       "  (fc3): Linear(in_features=208, out_features=104, bias=True)\n",
       "  (act3): Softmax(dim=None)\n",
       ")"
      ]
     },
     "execution_count": 83,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from src.models.pytorch import get_device\n",
    "\n",
    "device = get_device()\n",
    "model.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ClassificationEmbdNN(\n",
      "  (emb_layers): ModuleList(\n",
      "    (0): Embedding(5724, 1000)\n",
      "  )\n",
      "  (emb_dropout): Dropout(p=0.2, inplace=False)\n",
      "  (bn_cont): BatchNorm1d(4, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (fc1): Linear(in_features=1004, out_features=208, bias=True)\n",
      "  (dropout1): Dropout(p=0.2, inplace=False)\n",
      "  (bn1): BatchNorm1d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act1): ReLU()\n",
      "  (fc2): Linear(in_features=208, out_features=208, bias=True)\n",
      "  (dropout2): Dropout(p=0.2, inplace=False)\n",
      "  (bn2): BatchNorm1d(208, eps=1e-05, momentum=0.1, affine=True, track_running_stats=True)\n",
      "  (act2): ReLU()\n",
      "  (fc3): Linear(in_features=208, out_features=104, bias=True)\n",
      "  (act3): Softmax(dim=None)\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "criterion = torch.nn.CrossEntropyLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "BATCH_SIZE = 300\n",
    "N_EPOCHS = 10"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 84,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_dataset,batch_size=BATCH_SIZE)\n",
    "\n",
    "valid_loader = DataLoader(val_dataset,batch_size=BATCH_SIZE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [tensor([[1897.],\n",
       "          [4918.],\n",
       "          [1701.],\n",
       "          [1963.],\n",
       "          [4863.],\n",
       "          [5448.],\n",
       "          [ 836.],\n",
       "          [ 413.],\n",
       "          [ 370.],\n",
       "          [ 701.],\n",
       "          [1963.],\n",
       "          [2421.],\n",
       "          [2262.],\n",
       "          [3805.],\n",
       "          [5497.],\n",
       "          [ 726.],\n",
       "          [1198.],\n",
       "          [2368.],\n",
       "          [4729.],\n",
       "          [2235.],\n",
       "          [2859.],\n",
       "          [4598.],\n",
       "          [1963.],\n",
       "          [ 177.],\n",
       "          [2642.],\n",
       "          [4843.],\n",
       "          [2885.],\n",
       "          [ 688.],\n",
       "          [4074.],\n",
       "          [3431.],\n",
       "          [3906.],\n",
       "          [4576.],\n",
       "          [4794.],\n",
       "          [2880.],\n",
       "          [3315.],\n",
       "          [1335.],\n",
       "          [4794.],\n",
       "          [2019.],\n",
       "          [ 809.],\n",
       "          [3189.],\n",
       "          [4872.],\n",
       "          [3804.],\n",
       "          [5208.],\n",
       "          [5693.],\n",
       "          [3556.],\n",
       "          [3431.],\n",
       "          [ 154.],\n",
       "          [ 151.],\n",
       "          [  45.],\n",
       "          [4598.],\n",
       "          [2464.],\n",
       "          [4598.],\n",
       "          [1198.],\n",
       "          [3799.],\n",
       "          [2514.],\n",
       "          [3617.],\n",
       "          [1335.],\n",
       "          [2967.],\n",
       "          [5392.],\n",
       "          [2294.],\n",
       "          [3808.],\n",
       "          [5425.],\n",
       "          [4785.],\n",
       "          [ 916.],\n",
       "          [5080.],\n",
       "          [2642.],\n",
       "          [ 701.],\n",
       "          [5381.],\n",
       "          [5608.],\n",
       "          [ 176.],\n",
       "          [2885.],\n",
       "          [4538.],\n",
       "          [ 568.],\n",
       "          [5739.],\n",
       "          [4860.],\n",
       "          [5377.],\n",
       "          [ 596.],\n",
       "          [4743.],\n",
       "          [1963.],\n",
       "          [3841.],\n",
       "          [ 154.],\n",
       "          [4884.],\n",
       "          [3729.],\n",
       "          [4725.],\n",
       "          [4228.],\n",
       "          [ 413.],\n",
       "          [2601.],\n",
       "          [5392.],\n",
       "          [3799.],\n",
       "          [3841.],\n",
       "          [2262.],\n",
       "          [3526.],\n",
       "          [2915.],\n",
       "          [4950.],\n",
       "          [2112.],\n",
       "          [3996.],\n",
       "          [4642.],\n",
       "          [4489.],\n",
       "          [4598.],\n",
       "          [ 411.],\n",
       "          [2922.],\n",
       "          [3577.],\n",
       "          [5364.],\n",
       "          [ 834.],\n",
       "          [ 235.],\n",
       "          [4538.],\n",
       "          [1776.],\n",
       "          [3804.],\n",
       "          [4725.],\n",
       "          [3512.],\n",
       "          [2729.],\n",
       "          [3804.],\n",
       "          [1933.],\n",
       "          [  94.],\n",
       "          [3446.],\n",
       "          [5669.],\n",
       "          [4059.],\n",
       "          [ 151.],\n",
       "          [2207.],\n",
       "          [ 654.],\n",
       "          [1032.],\n",
       "          [3873.],\n",
       "          [2127.],\n",
       "          [ 701.],\n",
       "          [3512.],\n",
       "          [4743.],\n",
       "          [3465.],\n",
       "          [1168.],\n",
       "          [ 886.],\n",
       "          [2262.],\n",
       "          [  48.],\n",
       "          [ 707.],\n",
       "          [5675.],\n",
       "          [3227.],\n",
       "          [2321.],\n",
       "          [5448.],\n",
       "          [ 104.],\n",
       "          [3260.],\n",
       "          [1292.],\n",
       "          [5242.],\n",
       "          [3189.],\n",
       "          [ 163.],\n",
       "          [2470.],\n",
       "          [ 943.],\n",
       "          [5003.],\n",
       "          [1963.],\n",
       "          [4840.],\n",
       "          [2461.],\n",
       "          [4733.],\n",
       "          [ 505.],\n",
       "          [ 163.],\n",
       "          [2461.],\n",
       "          [4794.],\n",
       "          [5582.],\n",
       "          [4934.],\n",
       "          [5553.],\n",
       "          [5607.],\n",
       "          [2249.],\n",
       "          [2698.],\n",
       "          [5531.],\n",
       "          [4105.],\n",
       "          [2616.],\n",
       "          [5679.],\n",
       "          [2367.],\n",
       "          [5392.],\n",
       "          [1556.],\n",
       "          [5334.],\n",
       "          [2207.],\n",
       "          [1343.],\n",
       "          [5448.],\n",
       "          [2523.],\n",
       "          [1629.],\n",
       "          [1933.],\n",
       "          [ 413.],\n",
       "          [ 360.],\n",
       "          [2536.],\n",
       "          [5278.],\n",
       "          [1325.],\n",
       "          [4112.],\n",
       "          [4384.],\n",
       "          [5167.],\n",
       "          [3799.],\n",
       "          [1989.],\n",
       "          [3512.],\n",
       "          [ 654.],\n",
       "          [3194.],\n",
       "          [ 224.],\n",
       "          [3667.],\n",
       "          [ 413.],\n",
       "          [1258.],\n",
       "          [5112.],\n",
       "          [4538.],\n",
       "          [5414.],\n",
       "          [1344.],\n",
       "          [ 151.],\n",
       "          [1266.],\n",
       "          [3189.],\n",
       "          [5381.],\n",
       "          [2316.],\n",
       "          [3099.],\n",
       "          [ 892.],\n",
       "          [5112.],\n",
       "          [3239.],\n",
       "          [ 163.],\n",
       "          [5448.],\n",
       "          [3996.],\n",
       "          [5412.],\n",
       "          [ 375.],\n",
       "          [ 707.],\n",
       "          [ 701.],\n",
       "          [5078.],\n",
       "          [1198.],\n",
       "          [5527.],\n",
       "          [5582.],\n",
       "          [5132.],\n",
       "          [2249.],\n",
       "          [4389.],\n",
       "          [ 437.],\n",
       "          [2318.],\n",
       "          [ 993.],\n",
       "          [ 163.],\n",
       "          [4950.],\n",
       "          [2585.],\n",
       "          [4963.],\n",
       "          [ 235.],\n",
       "          [5531.],\n",
       "          [5242.],\n",
       "          [5607.],\n",
       "          [1893.],\n",
       "          [4524.],\n",
       "          [5448.],\n",
       "          [4435.],\n",
       "          [ 257.],\n",
       "          [ 411.],\n",
       "          [2911.],\n",
       "          [ 360.],\n",
       "          [4743.],\n",
       "          [4489.],\n",
       "          [ 342.],\n",
       "          [1933.],\n",
       "          [ 369.],\n",
       "          [4743.],\n",
       "          [ 235.],\n",
       "          [2514.],\n",
       "          [ 923.],\n",
       "          [2368.],\n",
       "          [5112.],\n",
       "          [5448.],\n",
       "          [1283.],\n",
       "          [3996.],\n",
       "          [1963.],\n",
       "          [4950.],\n",
       "          [4831.],\n",
       "          [2262.],\n",
       "          [4538.],\n",
       "          [1238.],\n",
       "          [2707.],\n",
       "          [5448.],\n",
       "          [5078.],\n",
       "          [2461.],\n",
       "          [4884.],\n",
       "          [5014.],\n",
       "          [4191.],\n",
       "          [5278.],\n",
       "          [2213.],\n",
       "          [ 104.],\n",
       "          [5242.],\n",
       "          [1238.],\n",
       "          [1254.],\n",
       "          [ 163.],\n",
       "          [ 701.],\n",
       "          [1341.],\n",
       "          [ 905.],\n",
       "          [3525.],\n",
       "          [ 260.],\n",
       "          [1963.],\n",
       "          [1933.],\n",
       "          [4831.],\n",
       "          [4598.],\n",
       "          [2562.],\n",
       "          [5014.],\n",
       "          [4538.],\n",
       "          [5224.],\n",
       "          [4831.],\n",
       "          [2853.],\n",
       "          [1963.],\n",
       "          [ 114.],\n",
       "          [2537.],\n",
       "          [2707.],\n",
       "          [ 965.],\n",
       "          [ 177.],\n",
       "          [ 505.],\n",
       "          [5132.],\n",
       "          [1286.],\n",
       "          [5553.],\n",
       "          [2238.],\n",
       "          [ 701.],\n",
       "          [1825.],\n",
       "          [ 802.],\n",
       "          [3804.]]),\n",
       "  tensor([[ 0.3790,  1.0686,  1.8415,  0.9661],\n",
       "          [-3.2047, -2.1777, -0.3572, -0.4001],\n",
       "          [-0.3378,  0.2570, -0.3572, -0.4001],\n",
       "          ...,\n",
       "          [ 0.3790,  0.2570,  0.3757,  0.2830],\n",
       "          [-1.7712, -0.5545, -2.5560, -2.4494],\n",
       "          [-0.3378, -0.5545,  0.3757, -0.4001]])],\n",
       " 'target': tensor([ 11,  13,  92,  18,  93,  21,  92,  98,  11,  73,   9,  82,  11,  11,\n",
       "          61,  31,  60,  78,   3,  12,  12, 102,  11,  84,   5,  81,  18,  12,\n",
       "          37,   3,  47, 102,  84,  19,  84,  39,  12,  51,  26,   4,  12,  98,\n",
       "          42,  43,  30,  83,  12,  42,  99,  83,  17,  46,  58, 103,  99,  14,\n",
       "          98,  92,  98,   6,   2,  19,  61,  86,  40,  84, 103,  36,  17,  18,\n",
       "          76,   9,  26,  21,  40,  11,  25,  12, 103,  61,  12,  47,   7,  46,\n",
       "          12,  37, 100,  98,  14,  89,  94,  14,  42,  89, 100,  36,  98,   4,\n",
       "          51,  94,  44,  14,   9,  89,   7,  26,  76,  12,  60,  18,  65,  35,\n",
       "          55,  49,  12,  49,  44,  33,  12,  14,  70,  98,  12,  92,  52,  12,\n",
       "          82,   5,  26,  14,  24,  68,  50,   9,   6,  14,  20,  19,  75,   9,\n",
       "          19,  13,  14,  24,   7, 103,  14,   4,  14,  98,  76,  14,  84,  79,\n",
       "          21,  42,  96,  21,   2,  83,  44,  49,  65,   2,  60,  14,  99,  19,\n",
       "          98,  98,  67,  93,  18,  16,  37,  57,  37,  26, 102,  82,  92,  58,\n",
       "          73,  61,  12,  14,   0,   1,   2,  98,  84,  20, 103, 103,   7,  25,\n",
       "           9,  24,  17,   6,  53,  92,  94,  40,  14,  14,  70,  12,  14,  31,\n",
       "           2,  86,  41,   2,  19,  12,  25,  17,  55,  80,  76,  14,  46,  73,\n",
       "          25,  79,  16,  96,  98,  55,  21,  43, 103,  43,  53,  40,  14,  12,\n",
       "          47,  11,  92,  12,  89,   1,  75,  35,  11,  80,  22,  89,  11,  19,\n",
       "           4,  11,  12,  89,   9,   9,  12,  40,  47,  11,  80,  37,  14,  26,\n",
       "          12,  42,  26,   1,  16,  98,  23,  47,  21,   9,  14, 103,  83,  43,\n",
       "          12,   9,   2,  21,  12,  18,   4,  82,  92,  61,  42,   7,   2,  74,\n",
       "          89,   2,  16,  17,  22,  44])}"
      ]
     },
     "execution_count": 85,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(train_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'data': [tensor([[4884.],\n",
       "          [ 163.],\n",
       "          [ 217.],\n",
       "          [ 163.],\n",
       "          [2262.],\n",
       "          [5378.],\n",
       "          [4280.],\n",
       "          [ 413.],\n",
       "          [5003.],\n",
       "          [2461.],\n",
       "          [2461.],\n",
       "          [2316.],\n",
       "          [3841.],\n",
       "          [5066.],\n",
       "          [4453.],\n",
       "          [2461.],\n",
       "          [3341.],\n",
       "          [ 151.],\n",
       "          [4830.],\n",
       "          [2239.],\n",
       "          [5014.],\n",
       "          [3237.],\n",
       "          [2514.],\n",
       "          [5378.],\n",
       "          [5014.],\n",
       "          [5448.],\n",
       "          [3189.],\n",
       "          [5242.],\n",
       "          [2523.],\n",
       "          [1605.],\n",
       "          [2601.],\n",
       "          [5364.],\n",
       "          [ 163.],\n",
       "          [5078.],\n",
       "          [ 224.],\n",
       "          [2689.],\n",
       "          [4132.],\n",
       "          [ 375.],\n",
       "          [ 360.],\n",
       "          [3888.],\n",
       "          [2239.],\n",
       "          [5392.],\n",
       "          [1334.],\n",
       "          [4598.],\n",
       "          [1951.],\n",
       "          [4680.],\n",
       "          [1306.],\n",
       "          [5330.],\n",
       "          [ 154.],\n",
       "          [3804.],\n",
       "          [4950.],\n",
       "          [ 386.],\n",
       "          [2927.],\n",
       "          [4794.],\n",
       "          [4228.],\n",
       "          [1776.],\n",
       "          [3843.],\n",
       "          [ 291.],\n",
       "          [2909.],\n",
       "          [1871.],\n",
       "          [4228.],\n",
       "          [2239.],\n",
       "          [3431.],\n",
       "          [ 509.],\n",
       "          [2262.],\n",
       "          [4453.],\n",
       "          [1168.],\n",
       "          [5370.],\n",
       "          [3057.],\n",
       "          [4538.],\n",
       "          [2989.],\n",
       "          [ 701.],\n",
       "          [3526.],\n",
       "          [ 587.],\n",
       "          [5726.],\n",
       "          [ 235.],\n",
       "          [1506.],\n",
       "          [1933.],\n",
       "          [4077.],\n",
       "          [4262.],\n",
       "          [4794.],\n",
       "          [ 615.],\n",
       "          [ 413.],\n",
       "          [ 177.],\n",
       "          [4743.],\n",
       "          [ 235.],\n",
       "          [2698.],\n",
       "          [4950.],\n",
       "          [2262.],\n",
       "          [2270.],\n",
       "          [2523.],\n",
       "          [3194.],\n",
       "          [4280.],\n",
       "          [3799.],\n",
       "          [4963.],\n",
       "          [3804.],\n",
       "          [3341.],\n",
       "          [5253.],\n",
       "          [1933.],\n",
       "          [ 911.],\n",
       "          [5003.],\n",
       "          [1338.],\n",
       "          [1285.],\n",
       "          [3099.],\n",
       "          [1899.],\n",
       "          [4832.],\n",
       "          [4031.],\n",
       "          [5448.],\n",
       "          [2927.],\n",
       "          [ 836.],\n",
       "          [4863.],\n",
       "          [3841.],\n",
       "          [ 829.],\n",
       "          [5242.],\n",
       "          [1306.],\n",
       "          [5448.],\n",
       "          [5112.],\n",
       "          [1776.],\n",
       "          [2642.],\n",
       "          [1303.],\n",
       "          [4950.],\n",
       "          [4280.],\n",
       "          [4611.],\n",
       "          [2967.],\n",
       "          [2262.],\n",
       "          [ 767.],\n",
       "          [3208.],\n",
       "          [4843.],\n",
       "          [ 154.],\n",
       "          [4357.],\n",
       "          [1314.],\n",
       "          [2927.],\n",
       "          [4226.],\n",
       "          [4748.],\n",
       "          [3809.],\n",
       "          [  48.],\n",
       "          [1325.],\n",
       "          [ 151.],\n",
       "          [1895.],\n",
       "          [ 413.],\n",
       "          [5112.],\n",
       "          [4794.],\n",
       "          [4412.],\n",
       "          [1515.],\n",
       "          [1719.],\n",
       "          [5278.],\n",
       "          [4537.],\n",
       "          [4141.],\n",
       "          [ 596.],\n",
       "          [4540.],\n",
       "          [2112.],\n",
       "          [4241.],\n",
       "          [3996.],\n",
       "          [4619.],\n",
       "          [2601.],\n",
       "          [5582.],\n",
       "          [1936.],\n",
       "          [3799.],\n",
       "          [5527.],\n",
       "          [3596.],\n",
       "          [4485.],\n",
       "          [2523.],\n",
       "          [ 104.],\n",
       "          [1558.],\n",
       "          [2412.],\n",
       "          [ 598.],\n",
       "          [2262.],\n",
       "          [ 235.],\n",
       "          [ 151.],\n",
       "          [4682.],\n",
       "          [4794.],\n",
       "          [ 440.],\n",
       "          [3788.],\n",
       "          [2512.],\n",
       "          [ 716.],\n",
       "          [ 411.],\n",
       "          [2262.],\n",
       "          [1238.],\n",
       "          [1168.],\n",
       "          [3608.],\n",
       "          [4624.],\n",
       "          [5332.],\n",
       "          [4384.],\n",
       "          [ 697.],\n",
       "          [1933.],\n",
       "          [4863.],\n",
       "          [ 151.],\n",
       "          [ 911.],\n",
       "          [4743.],\n",
       "          [2262.],\n",
       "          [ 831.],\n",
       "          [1325.],\n",
       "          [4748.],\n",
       "          [4241.],\n",
       "          [4489.],\n",
       "          [2318.],\n",
       "          [4353.],\n",
       "          [4447.],\n",
       "          [2464.],\n",
       "          [3804.],\n",
       "          [2417.],\n",
       "          [1473.],\n",
       "          [4863.],\n",
       "          [2523.],\n",
       "          [  57.],\n",
       "          [5448.],\n",
       "          [2464.],\n",
       "          [4950.],\n",
       "          [5242.],\n",
       "          [5003.],\n",
       "          [4733.],\n",
       "          [5563.],\n",
       "          [5392.],\n",
       "          [5485.],\n",
       "          [4838.],\n",
       "          [2967.],\n",
       "          [3512.],\n",
       "          [1776.],\n",
       "          [4794.],\n",
       "          [1198.],\n",
       "          [1198.],\n",
       "          [4031.],\n",
       "          [2529.],\n",
       "          [3596.],\n",
       "          [3189.],\n",
       "          [2537.],\n",
       "          [ 151.],\n",
       "          [2601.],\n",
       "          [ 413.],\n",
       "          [4489.],\n",
       "          [1728.],\n",
       "          [2536.],\n",
       "          [1414.],\n",
       "          [2262.],\n",
       "          [5278.],\n",
       "          [ 413.],\n",
       "          [5003.],\n",
       "          [4972.],\n",
       "          [1318.],\n",
       "          [1963.],\n",
       "          [2668.],\n",
       "          [3804.],\n",
       "          [3872.],\n",
       "          [4838.],\n",
       "          [ 163.],\n",
       "          [4963.],\n",
       "          [5392.],\n",
       "          [2464.],\n",
       "          [ 151.],\n",
       "          [ 701.],\n",
       "          [1314.],\n",
       "          [4863.],\n",
       "          [ 670.],\n",
       "          [1342.],\n",
       "          [5737.],\n",
       "          [3906.],\n",
       "          [5553.],\n",
       "          [4489.],\n",
       "          [4489.],\n",
       "          [ 413.],\n",
       "          [ 163.],\n",
       "          [ 363.],\n",
       "          [3847.],\n",
       "          [5527.],\n",
       "          [4950.],\n",
       "          [1933.],\n",
       "          [5529.],\n",
       "          [3677.],\n",
       "          [3624.],\n",
       "          [1198.],\n",
       "          [1330.],\n",
       "          [1334.],\n",
       "          [3853.],\n",
       "          [ 515.],\n",
       "          [5132.],\n",
       "          [ 704.],\n",
       "          [4838.],\n",
       "          [4838.],\n",
       "          [3294.],\n",
       "          [4489.],\n",
       "          [  97.],\n",
       "          [1198.],\n",
       "          [ 701.],\n",
       "          [3189.],\n",
       "          [4950.],\n",
       "          [4939.],\n",
       "          [3128.],\n",
       "          [3692.],\n",
       "          [4743.],\n",
       "          [ 767.],\n",
       "          [2262.],\n",
       "          [ 701.],\n",
       "          [5115.],\n",
       "          [ 596.],\n",
       "          [4729.],\n",
       "          [ 819.],\n",
       "          [5364.],\n",
       "          [ 707.],\n",
       "          [2461.],\n",
       "          [1238.]]),\n",
       "  tensor([[ 0.3790,  0.2570,  0.3757,  0.2830],\n",
       "          [-1.0545, -0.5545, -1.0901, -1.0832],\n",
       "          [-0.3378,  0.2570, -0.3572,  0.2830],\n",
       "          ...,\n",
       "          [-0.3378,  1.0686, -1.8230, -1.0832],\n",
       "          [ 0.3790,  1.0686,  1.1086,  0.9661],\n",
       "          [-0.3378,  0.2570,  0.3757, -0.4001]])],\n",
       " 'target': tensor([ 79,  31,  80,  15,  84,   0,  65,  11,  79,  23,  89,  93,  61,  27,\n",
       "          53,  84,  65,  33,  14, 103,  86,  44,  80,   0,  12,   9,  19,  14,\n",
       "          68,   1,  16,  98,  77,  14,   3,  18,  84,  12,  81,  44,  17,  98,\n",
       "          26,  46,  82,  17,  74,  17,  16,  20,   2,  66,   3,  14,   9,  76,\n",
       "          14,   2,  44,  60,  89,  21,  60,  12,  11,  31,   7,  17,  15,  24,\n",
       "          65,  31,   2,  61,  36,   4,  79,  14,   1,  92,  86,  47,  14,   2,\n",
       "          14,  67,  16,  17,  90,  83,  67,  35,  37,  24,  17,  60,  65,  32,\n",
       "          42,  24,  12,  58,  26,  17,   7,   6,  13, 103,  60,  92,  67,  26,\n",
       "          25,   9,  74,  12,  66,  16,  42,  26,  17,  37,  18,  29,  94,  74,\n",
       "          37,  37,  16,  61,  63,  60,  36,  55,  45,  24,  39,  31,  58,  16,\n",
       "          25,  17,  36,  47,  23,  37,  94,   6,  98,  12,  65,  12,   9,  20,\n",
       "           9,  16,   1,  16,  65,  51,  16,  68,  39,  14,  11,  14,  90,  86,\n",
       "         102, 102,  99,  61,   2,  52,  18,  94,  94,  89,  17,  70,  69,  49,\n",
       "          55,  14,   9,   1, 102,  24,  86,  94,  92,  39,  94,  98,  66,  49,\n",
       "           5,  90,  17,  40,  17,  89,  78,  77,  76,  12,  68,  19,  89,  43,\n",
       "          12,  26,  98,  90,  19,  25,  68,  76,  94,  22,  39,   1,  47,  51,\n",
       "           2,  81, 102,  16, 103,   7,  82,  89,  60,  11,  37,  19,  43,   1,\n",
       "          58,  25,   9,  60,   4,  12,  76,  42,  25,  31,  17,   4,  63,  37,\n",
       "          36,  24,  37,  20,   9,  60,  16,  89,   3,  12,   5,  65,   5,  18,\n",
       "          89,  14,  59,  29,  75,  86,   9,  77,  42,  89,  19,   2,  84,  83,\n",
       "           9,  39,  82,  16,  14,  16,  84,   1,  12,  74,   9,  60,  44,  25,\n",
       "          66,  11,   0,   9,  89,  90])}"
      ]
     },
     "execution_count": 86,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "next(iter(valid_loader))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm import tqdm_notebook as tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_network(model, train_loader, valid_loader,\n",
    "                  loss_func, optimizer, n_epochs=20,\n",
    "                  saved_model='model.pt'):\n",
    "    \n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    model.to(device)\n",
    "    \n",
    "    train_losses = list()\n",
    "    valid_losses = list()\n",
    "    \n",
    "    valid_loss_min = np.Inf\n",
    "    \n",
    "    for epoch in range(n_epochs):\n",
    "        train_loss = 0.0\n",
    "        valid_loss = 0.0\n",
    "        \n",
    "#         train_auc = 0.0\n",
    "#         valid_auc = 0.0\n",
    "        \n",
    "        train_acc = 0.0\n",
    "        valid_acc = 0.0\n",
    "        \n",
    "        model.train()\n",
    "        for batch in tqdm(train_loader):\n",
    "            optimizer.zero_grad()\n",
    "            \n",
    "            output = model(batch['data'][0].to(device, \n",
    "                                               dtype=torch.long),\n",
    "                           batch['data'][1].to(device, \n",
    "                                               dtype=torch.float))\n",
    "            \n",
    "            \n",
    "            loss = loss_func(output, batch['target'].to(device, \n",
    "                                                        dtype=torch.long))\n",
    "            \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            # Calculate global accuracy\n",
    "            train_acc += (output.argmax(1) == batch['target']).sum().item()\n",
    "#             train_auc += roc_auc_score(batch['target'].cpu().numpy(),\n",
    "#                                                output.detach().cpu().numpy(),\n",
    "#                                                multi_class = \"ovo\")\n",
    "\n",
    "            train_loss += loss.item() * batch['data'][0].size(0)  #!!!\n",
    "    \n",
    "\n",
    "        model.eval()\n",
    "        for batch in tqdm(valid_loader):\n",
    "            output = model(batch['data'][0].to(device, \n",
    "                                               dtype=torch.long),\n",
    "                           batch['data'][1].to(device, \n",
    "                                               dtype=torch.float))\n",
    "            \n",
    "            \n",
    "            loss = loss_func(output, batch['target'].to(device, \n",
    "                                                        dtype=torch.long))\n",
    "            \n",
    "#             valid_auc += roc_auc_score(batch['target'].cpu().numpy(),\n",
    "#                                                output.detach().cpu().numpy(),\n",
    "#                                                multi_class = \"ovo\")\n",
    "            valid_loss += loss.item() * batch['data'][0].size(0)  #!!!\n",
    "            # Calculate global accuracy\n",
    "            valid_acc += (output.argmax(1) == batch['target']).sum().item()\n",
    "        \n",
    "#         train_loss = np.sqrt(train_loss / len(train_loader.sampler.indices))\n",
    "#         valid_loss = np.sqrt(valid_loss / len(valid_loader.sampler.indices))\n",
    "\n",
    "#         train_auc = train_auc / len(train_loader)\n",
    "#         valid_auc = valid_auc / len(valid_loader)\n",
    "        \n",
    "#         train_losses.append(train_loss)\n",
    "#         valid_losses.append(valid_loss)\n",
    "\n",
    "        print('Epoch: {}. Training loss: {:.6f}. Validation loss: {:.6f}'\n",
    "              .format(epoch, train_loss, valid_loss))\n",
    "        print('Training AUC: {:.6f}. Validation AUC: {:.6f}'\n",
    "              .format(train_acc, valid_acc))\n",
    "        \n",
    "        if valid_loss < valid_loss_min:  # let's save the best weights to use them in prediction\n",
    "            print('Validation loss decreased ({:.6f} --> {:.6f}). Saving model...'\n",
    "                  .format(valid_loss_min, valid_loss))\n",
    "            \n",
    "            torch.save(model.state_dict(), saved_model)\n",
    "            valid_loss_min = valid_loss\n",
    "            \n",
    "    \n",
    "    return train_losses, valid_losses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:24: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5c6281a159dc4acda555554e610254c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3174.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:48: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d0dee03b97594caab868a35abf6f7fe8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1058.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 0. Training loss: 4421724.604657. Validation loss: 1473862.318449\n",
      "Training AUC: 5908.000000. Validation AUC: 2346.000000\n",
      "Validation loss decreased (inf --> 1473862.318449). Saving model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "76622ec8d3e44ed8b7d24608b1dbcb46",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3174.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e448e268b0b0414a9ed166054c16bd60",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1058.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 1. Training loss: 4421724.043646. Validation loss: 1473859.110718\n",
      "Training AUC: 5853.000000. Validation AUC: 2390.000000\n",
      "Validation loss decreased (1473862.318449 --> 1473859.110718). Saving model...\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "d2847909bf044a869c9c63438eafadfc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3174.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "63d2d17a48df4bb8ae28f66dd2124838",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1058.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 2. Training loss: 4421721.100763. Validation loss: 1473863.261547\n",
      "Training AUC: 5896.000000. Validation AUC: 2349.000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "21e4fdcb2b6d40f591cdfcd3aa7ec956",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3174.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c7bfc3b55cc34c648d1af1764ac5bb15",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1058.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 3. Training loss: 4421726.304363. Validation loss: 1473860.931282\n",
      "Training AUC: 5861.000000. Validation AUC: 2359.000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa67e105d6234870a7ebe5a0846fbb94",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3174.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "95e28d89795b45209cbcb31aa8949a8f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1058.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 4. Training loss: 4421723.953760. Validation loss: 1473859.757814\n",
      "Training AUC: 5849.000000. Validation AUC: 2359.000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c3b0017a4ef4d73bdb1bfab01bad09b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3174.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1021df4bcb794dbf9cd58961e9582e78",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1058.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 5. Training loss: 4421724.728816. Validation loss: 1473861.321592\n",
      "Training AUC: 5849.000000. Validation AUC: 2317.000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "aa48da87b5914108a1cd65f0dcfa73f1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3174.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "71ad57026534429b94d2c8826e6458f3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1058.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 6. Training loss: 4421720.038199. Validation loss: 1473861.021938\n",
      "Training AUC: 5697.000000. Validation AUC: 2417.000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "05e7e26de827450e9c0095906c11ffc5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3174.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "768a1a632c64478287cb487d5008f5a1",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1058.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 7. Training loss: 4421728.686065. Validation loss: 1473860.797329\n",
      "Training AUC: 5810.000000. Validation AUC: 2387.000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ea9ebd0bcf714d9ca6b061c269bc0f1e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3174.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "35f09b60c7404100a972d22fa5cfe619",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1058.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 8. Training loss: 4421722.932936. Validation loss: 1473860.643997\n",
      "Training AUC: 5801.000000. Validation AUC: 2354.000000\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "42c8a85a7acf484394e6b07784e799b8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=3174.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3758099e398047379197efa3a5f0e401",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1058.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Epoch: 9. Training loss: 4421724.683504. Validation loss: 1473858.867426\n",
      "Training AUC: 5926.000000. Validation AUC: 2361.000000\n",
      "Validation loss decreased (1473859.110718 --> 1473858.867426). Saving model...\n"
     ]
    }
   ],
   "source": [
    "train_losses, valid_losses = train_network(model=model, \n",
    "                                           train_loader=train_loader, \n",
    "                                           valid_loader=valid_loader, \n",
    "                                           loss_func=criterion, \n",
    "                                           optimizer=optimizer,\n",
    "                                           n_epochs=N_EPOCHS, \n",
    "                                           saved_model='../models/embed_3layers.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### forgot to divide the loss and accuracy by length of data set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Accuracy: 19.75%\n",
      "Validation Accuracy: 7.87%\n"
     ]
    }
   ],
   "source": [
    "print('Training Accuracy: {:.2f}%'.format(5926.0/300.0))\n",
    "print('Validation Accuracy: {:.2f}%'.format(2361.0/300.0))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict with test set"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 108,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(data_loader, model):\n",
    "    model.eval()\n",
    "    device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\n",
    "    \n",
    "    model.to(device)\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        predictions = None\n",
    "        \n",
    "        for i, batch in enumerate(tqdm(data_loader)):   \n",
    "            \n",
    "            output = model(batch['data'][0].to(device, \n",
    "                                               dtype=torch.long), \n",
    "                           batch['data'][1].to(device, \n",
    "                                               dtype=torch.float)).cpu().numpy()\n",
    "            \n",
    "            if i == 0:\n",
    "                predictions = output\n",
    "                \n",
    "            else: \n",
    "                \n",
    "                predictions = np.vstack((predictions, output))\n",
    "                \n",
    "    return predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:10: TqdmDeprecationWarning: This function will be removed in tqdm==5.0.0\n",
      "Please use `tqdm.notebook.tqdm` instead of `tqdm.tqdm_notebook`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1828c3cc067547f3803e436fb770e7bc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "HBox(children=(FloatProgress(value=0.0, max=1058.0), HTML(value='')))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/opt/conda/lib/python3.7/site-packages/ipykernel_launcher.py:73: UserWarning: Implicit dimension choice for softmax has been deprecated. Change the call to include dim=X as an argument.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "model.load_state_dict(torch.load('../models/embed_3layers.pt'))\n",
    "\n",
    "test_loader = DataLoader(test_dataset, \n",
    "                         batch_size=BATCH_SIZE)\n",
    "\n",
    "nn_predictions = predict(test_loader, model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.01260628, 0.00742896, 0.00644201, ..., 0.00936545, 0.01223338,\n",
       "        0.00800614],\n",
       "       [0.00870188, 0.00563232, 0.01582166, ..., 0.0086471 , 0.01082051,\n",
       "        0.0085842 ],\n",
       "       [0.00938711, 0.0065556 , 0.01092372, ..., 0.00528657, 0.00847999,\n",
       "        0.01576285],\n",
       "       ...,\n",
       "       [0.01357877, 0.00672801, 0.0100839 , ..., 0.00834409, 0.00656633,\n",
       "        0.01293461],\n",
       "       [0.00949264, 0.00695431, 0.01194292, ..., 0.01046819, 0.00798348,\n",
       "        0.00741773],\n",
       "       [0.01299331, 0.00624016, 0.01440495, ..., 0.00893319, 0.00758309,\n",
       "        0.01268085]], dtype=float32)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nn_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_acc = (nn_predictions.argmax(1) == y_test).sum().item()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "7.533333333333333"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_acc/300"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score, classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4919978611119817"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# compute other metrics\n",
    "roc_auc_score(y_test,nn_predictions, multi_class='ovr', average='macro')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[ 83 102  11 ...  43   9  14]\n",
      "[73 50 80 ... 12 32 99]\n"
     ]
    }
   ],
   "source": [
    "print(y_test)\n",
    "print(nn_predictions.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_cr_to_dataframe(report_dict:dict) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Converts the dictionary format of the Classification Report (CR) to a\n",
    "    dataframe for easy of sorting\n",
    "    :param report_dict: The dictionary returned by \n",
    "    sklearn.metrics.classification_report.\n",
    "    :return: Returns a dataframe of the same information.\n",
    "    \"\"\"\n",
    "    beer_style = list(report_dict.keys())\n",
    "    beer_style.remove('accuracy')\n",
    "    beer_style.remove('macro avg')\n",
    "    beer_style.remove('weighted avg')\n",
    "    precision = []\n",
    "    recall = []\n",
    "    f1 = []\n",
    "    support = []\n",
    "    for key, value in report_dict.items():\n",
    "        if key not in ['accuracy', 'macro avg', 'weighted avg']:\n",
    "            precision.append(value['precision'])\n",
    "            recall.append(value['recall'])\n",
    "            f1.append(value['f1-score'])\n",
    "            support.append(value['support'])\n",
    "    result = pd.DataFrame({'beer_style': beer_style,\n",
    "                           'precision': precision,\n",
    "                           'recall': recall,\n",
    "                           'f1': f1,\n",
    "                           'support': support})\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [],
   "source": [
    "from joblib import load\n",
    "\n",
    "lbel_encoders = load('../models/label_encoders.joblib')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                     beer_style  precision    recall        f1  support\n",
      "0                       Altbier   0.004481  0.013342  0.006709     1574\n",
      "1        American Adjunct Lager   0.000000  0.000000  0.000000     6047\n",
      "2      American Amber / Red Ale   0.031808  0.013229  0.018686     9298\n",
      "3    American Amber / Red Lager   0.141541  0.091302  0.111002     1851\n",
      "4           American Barleywine   0.001211  0.000184  0.000319     5439\n",
      "..                          ...        ...       ...       ...      ...\n",
      "99                 Vienna Lager   0.000386  0.000548  0.000453     1825\n",
      "100                  Weizenbock   0.000000  0.000000  0.000000     1900\n",
      "101                   Wheatwine   0.000000  0.000000  0.000000      769\n",
      "102               Winter Warmer   0.000000  0.000000  0.000000     4140\n",
      "103                     Witbier   0.018517  0.052798  0.027419     6023\n",
      "\n",
      "[104 rows x 5 columns]\n"
     ]
    }
   ],
   "source": [
    "report_dict = classification_report(label_encoders['beer_style'].inverse_transform(y_test),\n",
    "                                    label_encoders['beer_style'].inverse_transform(nn_predictions.argmax(1)),\n",
    "                                    output_dict=True)\n",
    "report_df = convert_cr_to_dataframe(report_dict)\n",
    "print(report_df)\n",
    "#classification_report(y_test, nn_predictions.argmax(1))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model, \"../models/model.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
